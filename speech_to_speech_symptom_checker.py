# -*- coding: utf-8 -*-
"""Speech-to-Speech Symptom Checker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E0B9YnTzGCleCACRwq_JpNNk5ai0V-75
"""

from huggingface_hub import notebook_login
notebook_login()

#Installing all requirements
!pip install langchain
!pip install -U langchain-community
!pip install sentence-transformers
!pip install faiss-cpu
!pip install groq
!pip install torchaudio
!pip install gTTS
!pip install soundfile
!pip install playsound

from google.colab import files
uploaded = files.upload()

#Formatting the data
import pandas as pd
df=pd.read_csv('Diseases_Symptoms.csv')
df.head()

formatted_data = []
for index, row in df.iterrows():
    formatted_text = f"Disease: {row['Name']}. Symptoms: {row['Symptoms']}. Recommendations: {row['Treatments']}."
    formatted_data.append(formatted_text)

for text in formatted_data[:5]:
    print(text)

from sentence_transformers import SentenceTransformer
embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

embeddings = embedding_model.encode(formatted_data)
print(f"Embeddings shape: {embeddings.shape}")

#Creating vector store
import faiss

# Dimensions of our embeddings
d = embeddings.shape[1]

# Creating an index for our dense vectors
index = faiss.IndexFlatL2(d)  # Using L2 (Euclidean) distance

# Adding the embeddings to the index
index.add(embeddings)

print(f"Total sentences indexed: {index.ntotal}")

from transformers import pipeline
from google.colab import userdata
from groq import Groq
from gtts import gTTS
from IPython.display import Audio

client = Groq(
    api_key=userdata.get('GROQ_API_KEY')
)

#Automatic speech recognization
asr = pipeline("automatic-speech-recognition", model="distil-whisper/distil-large-v2",device="cpu")
patient_prompt_dict = asr("record2.aac")
patient_prompt = patient_prompt_dict["text"] # Extract the text from the dictionary
print("You said:", patient_prompt)

#Converting the user prompt into embedding
query_embedding = embedding_model.encode([patient_prompt])

# Perform FAISS search
k = 2
distances, indices = index.search(query_embedding, k)
retrieved_info = [formatted_data[idx] for idx in indices[0]]
context = "\n".join(retrieved_info)


system_prompt = f"""You are an AI medical assistant. Based on the following information:{context} Generate a helpful and empathetic reply to the user query:
'{patient_prompt}'.If you think the condition is serious, then immediately suggest the patient to call emergency or else no need.If you are recommending any
medication, then mention to consult a doctor before taking it. Do ask the patient if they are experiencing any symptom that what type of symptom is it for
example cough now your job is to ask the cough is dry or producing phlegm or sputum. You also have to ask since when they are experiencing this problem if
they do not tell you. You will also ask if the symptom is triggered or worsen by any action or effect of anything, asking this helps in better diagnosis.
You will also ask that if they are experiencing any other symptoms for example a patient says that he has pain in his stomach so will ask that if he is
feeling nauseous or having fever, asking this also help in better diagnosis."""
chat_completion = client.chat.completions.create(
messages=[
        {
            "role": "system",
            "content": system_prompt,
        },
        {
            "role":"user",
            "content": patient_prompt
        }
    ],
    model="llama-3.3-70b-versatile",
    )
model_output=chat_completion.choices[0].message.content

tts = gTTS(model_output)
tts.save("tts1.mp3")
Audio("tts1.mp3", autoplay=True)